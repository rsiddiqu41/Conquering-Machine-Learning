{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Recap\n",
        "\n",
        "In the last lesson, we talked about machine learning and how we pass in data to train a model and make an accurate prediction based off that data. We talked about the different kinds of ways we can train the model with **supervised** and **unsupervised** learning. We also went over **regression** and how the goal of our model is to choose parameters that will minimize the value we get from our **loss functions**.\n",
        "\n",
        "In this lesson, we will talk about an algorithm called **Gradient Descent**, which is an algorithm we use to choose parameters that minimize loss from our cost (loss) function. This will help increase the accuracy of our predictions.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#**Understanding Gradient Descent**\n",
        "\n",
        "In our example from the last lesson, we talked about linear regression and we formulated a regession equation. If our dataset had **n** input features, then it would look like this:\n",
        "\n",
        "###**$h_\\theta$($x_i$) = $\\theta_0$ + $\\theta_1$$x_1$ + $\\theta_2$$x_2$ + .. + $\\theta_n$$x_n$**  \n",
        "\n",
        "Now our goal is to understand how Gradient Descent chooses the values for the parameters $\\theta_0$, $\\theta_1$, $\\theta_2$, and so on. For this example, we can start by initializing all the parameters to 0 as follows:\n",
        "\n",
        "###$\\overrightarrow{\\theta} = 0$\n",
        "\n",
        "This notation means that the vector $\\theta$ = 0, so all the parameters are 0. If we plug in 0 for our $\\theta$ vector, it will cause every prediction from our regression model to be 0. Now we take our predicted value $h_\\theta(x_i)$ and plug it into our regression loss function:\n",
        "\n",
        "###**J($\\theta$) = $\\frac{1}{2n}$$\\sum_{i=1}^n (0-y_i)^2$** $\\to$ **$\\frac{1}{2n}$$\\sum_{i=1}^n (-y_i)^2$**\n",
        "\n",
        "Plugging in the prices from our example for $y_i$, it will lead to a very high number, which means the prediction from our algorithm is very different from the actual output, making our model inaccurate.\n",
        "\n",
        "We perform this first iteration where we use an initial value for our parameters. After our first iteration, **Gradient Descent** will take a step in the direction that lowers the loss function output. It will use a **step size** to adjust the magnitude of the parameters and move the parameters in each direction to try to find where **J($\\theta$)** is the lowest.\n",
        "\n",
        "In computer science, this reminded me of the DFS algorithm, where we search all paths to find where **J($\\theta$)** is the smallest. We then adjusted our parameters and optimize our algorithm.\n",
        "\n",
        "Gradient Descent will continuously perform this process until we find the smallest output from our loss function.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#**Quantifying Gradient Descent**\n",
        "\n",
        "This is where the math starts to get a little tricky but we will get through it.\n",
        "\n",
        "Let's write a formula to quantify how one step of gradient descent gets updated:\n",
        "\n",
        "###**$\\theta_j$ := $\\theta_j$ - $\\alpha$$\\frac{d}{d\\theta}$J($\\theta$)**\n",
        "\n",
        "This looks a bit confusing but lets break it down. First lets look at the **:=** operator. This is called the **assignment operator**. This means that we will be assigning our variable on the left hand side with whatever output we get from the right hand side. This differs from the regular equal sign as the regular equal sign is just an assertion that the value on the left and right are equal.\n",
        "\n",
        "$\\alpha$ is called our **learning rate**. More on this later.\n",
        "\n",
        "$\\frac{d}{d\\theta}$J($\\theta$) is the partial derivative of our cost function, **J($\\theta$)**, with respect to parameter $\\theta_j$. So for instance, in our home price example from the previous lesson, if we are trying to optimize our first parameter $\\theta_1$, we would take the partial derivative of our cost function with respect to $\\theta_1$. Hopefully this makes sense, if not, I will go over why we use derivatives, but I highly recommend learning about partial derivatives [here](https://www.youtube.com/watch?v=JAf_aSIJryg).\n",
        "\n",
        "##**Why do we need to take the derivative of a function**\n",
        "\n",
        "In calculus, we take the derivative of the function to find the rate at which a variable changes. We also use the derivative to find the slope to the curve at a specific point. In this case, we will use derivatives to find the point where the slope (steepness) is smallest. The point at which the slope is the lowest is where our cost function will produce the lowest value. If we have data sets where we are dealing with multiple dimensions (multiple input features), we need to use partial derivatives the optimize each parameter.\n",
        "\n",
        "##**Deriving the gradient descent step function**\n",
        "\n",
        "Since we defined our gradient descent equation to show how the a parameter $\\theta_j$ gets updated one step at a time, let's perform the partial derivative to simplify our equation. Let's plugin our cost function for **J($\\theta$)** below:\n",
        "\n",
        "###**$\\frac{d}{d\\theta}$J($\\theta$) \t$\\to$ $\\frac{d}{d\\theta}$$\\frac{1}{2n}$$\\sum_{i=1}^n (h_\\theta(x_i)-y_i)^2$**\n",
        "\n",
        "For simplicity, let's calculate the partial derivative at a point n = j, where j is any single input data point (any row from our table):\n",
        "\n",
        "###$\\frac{d}{d\\theta_j}$$\\frac{1}{2n}$$\\sum_{i=1}^n (h_\\theta(x_i)-y_i)^2$ $\\to$ $\\frac{d}{d\\theta_j}$$\\frac{1}{2} (h_\\theta(x)-y)^2$\n",
        "\n",
        "using the chain rule and to $\\theta_j$ gives us:\n",
        "\n",
        "###2*$\\frac{1}{2} (h_\\theta(x)-y)$ * $\\frac{d}{d\\theta_j}$$(h_\\theta(x)-y)$\n",
        "\n",
        "now we can simplify plug in our regression equation for $h_\\theta(x)$:\n",
        "\n",
        "###$(h_\\theta(x)-y)$ $\\frac{d}{d\\theta_j}$$(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + .. + \\theta_nx_n-y_i)$\n",
        "\n",
        "now we can take the partial derivative with respect to $\\theta_j$, so all other $\\theta$ terms are treated as constants and set to 0. This will cancel all the other terms and taking the derivative of $\\theta_j$ will be 1. This will leave us with:\n",
        "\n",
        "###$(h_\\theta(x)-y)$ * $x_j$\n",
        "\n",
        "Now we can take this and plug it back into our original formula so:\n",
        "\n",
        "###**$\\theta_j$ := $\\theta_j$ - $\\alpha$$\\frac{d}{d\\theta}$J($\\theta$)**\n",
        "becomes\n",
        "\n",
        "###**$\\theta_j$ := $\\theta_j$ - $\\alpha$ $(h_\\theta(x)-y)$ * $x_j$**\n",
        "\n",
        "This equation represents how with one training example **j**, we can update our parameter $\\theta$.\n",
        "\n",
        "Since we have **n** training examples, this becomes:\n",
        "\n",
        "###**$\\theta_j$ := $\\theta_j$ - $\\alpha$ $\\sum_{i=1}^n$$(h_\\theta(x^i)-y^i)$ * $x_j^i$**\n",
        "\n",
        "where **i** represents the ith training example, **j** is the jth input feature.\n",
        "\n",
        "I know this was a lot of math but the main idea is that gradient descent will perform our derived function until convergence. This means that gradient descent will compute this function for j = 0, 1, 2, ..., n. It will do this over and over until we find the global minimum.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Learning Rate**\n",
        "\n",
        "The Learning Rate $\\alpha$ is a hyperparameter used in our gradient descent algorithm. If we take a look at our derived equation, we can see that the larger the learning rate, the bigger the size of the step we take during our descent. This means that **the learning rate controls how much our model's parameters are adjusted during each step of gradient descent**. We can use the learning rate to adjust the size of the steps during gradient descent, and the goal is to use it to minimize the number of steps taken to find the minimum of the loss function.\n",
        "\n",
        "###**Choosing a learning rate**\n",
        "\n",
        "Choosing a good value for the learning rate is very crucial as a very large learning rate can cause our step sizes during descent to be too large, causing us to overshoot the optimal parameter value and even diverge, leading to an unstable model.\n",
        "\n",
        "If the rate is too low, we will run way too many iterations of gradient descent and this will make our model converge very slowly. In some cases, this can cause us to miss the global minimum we are looking for as we may just choose a local minima.\n",
        "\n",
        "In practice, **many data scientists and ML engineers start off with a learning rate of 0.01 and then adjust and play around with it to see which value might be best**. Andrew Ng, one of the most famous ML engineers, likes to follow a quadratic method for testing which learning rate works best.\n",
        "\n",
        "For instance, we can start at 0.01, then increase it to 0.02, then 0.04, 0.08, 0.16, and so on.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "###**Batch Gradient Descent**\n",
        "\n",
        "So far, we have discussed gradient descent in the manner that if we have **n** training examples in our dataset, each step of gradient descent will run **n** times to choose the correct value for the parameter **j**.\n",
        "\n",
        "Now let's take a scenario where we have a very large dataset. Let's say n = 10000000 training examples, we would have to run our gradient descent algorithm 10000000 times **per step**. This would make gradient descent extremely slow as you are reading over every training example in the dataset everytime you perform a step in gradient descent. This type of gradient descent is called **Batch Gradient Descent**.\n",
        "\n",
        "This can be a disadvantage as adjusting just one of the parameters or taking just one step of gradient descent, we would have to run through our entire dataset and run the parameter update function on each one, which can be very slow and computationally expensive.\n",
        "\n",
        "###**Stochastic Gradient Descent**\n",
        "\n",
        "The alternative to batch gradient descent is **Stochastic Gradient Descent**. Instead of going through the entire dataset to run a step of gradient descent, we can just use one training example to determine our parameter values. This won't find the global minimum like it will in batch gradient descent but if done correctly, it can help us find a value that works. One way this is done in practice is by adjusting the learning rate.\n",
        "\n",
        "For instance, in our house dataset example we would calculate the gradient using one house at a time instead of using every house to calculate the gradient. We could also lower the learning rate if to test out if it would minimize our cost.\n",
        "\n",
        "###**Choosing Batch vs Stochastic**\n",
        "\n",
        "Batch gradient descent does a much better job of finding the values that minimize our cost function so it is preferred to use batch when possible. In practice today, many datasets are very large and running batch gradient descent on a TB of data is extremely costly and slow and wouldn't be ideal. In those cases, we can run stochastic gradient descent.\n",
        "\n",
        "###**So when do we stop training our model?**\n",
        "\n",
        "We can plot **J($\\theta$)** vs time to see if our model converges as time goes on. If it starts to converge, we can stop training our model.\n",
        "\n",
        "---\n",
        "\n",
        "###**Summary**\n",
        "\n",
        "**Gradient Descent** is an algorithm used to choose the parameters that minimize our cost function. Each parameter is calculated using the following equation:\n",
        "\n",
        "###**$\\theta_j$ := $\\theta_j$ - $\\alpha$$\\frac{d}{d\\theta}$J($\\theta$)**\n",
        "\n",
        "where we take the gradient (partial derivative) of our cost function with respect to $\\theta$.\n",
        "\n",
        "In **Batch Gradient Descent**, we perform this parameter update over the entire training set. In **stochastic gradient descent**, we perform this parameter update using a single training example from our dataset.\n",
        "\n",
        "If we have a very large dataset, we tend to use stochastic gradient descent as it is very costly and slow to perform batch gradient descent with billions of training examples."
      ],
      "metadata": {
        "id": "3QWjQUtMNI9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nlFaDnD6NHGi"
      }
    }
  ]
}