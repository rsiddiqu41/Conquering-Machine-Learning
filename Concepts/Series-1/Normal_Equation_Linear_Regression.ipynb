{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Recap\n",
        "\n",
        "In the last lesson, we talked about **Gradient Descent**, which is an algorithm we use to choose parameters that minimize loss from our cost (loss) function. This will help increase the accuracy of our predictions.\n",
        " We mainly went over the derivation of using gradient descent for **linear regression**.\n",
        "\n",
        " In this lesson, we will talk about how we can use the normal equation in linear regression to bypass gradient descent and perform much quicker optimizations.\n",
        "\n",
        " **Note**: This lecture requires a deeper understanding of matrix math and linear algebra for deriving equations.\n",
        "\n",
        "\n",
        "---\n",
        "##**Bypassing Gradient Descent in Linear Regression**\n",
        "\n",
        "The information I cover in this lecture apply to **Linear Regression Only!** If you take a look at the past couple of lectures, we break down linear regression to derive our gradient descent algorithm that we use to optimize our parameters $\\theta_j$. We talked about how batch gradient descent can be very slow for linear regression if we have a very large dataset. There is actually a way we can bypass gradient descent in linear regression and optimize our parameters in just one step, eliminating the need for using an iterative algorithm like gradient descent. This is called the **Normal Equation**.\n",
        "\n",
        "Note again, the normal equation is used for linear regression and linear regression only.\n",
        "\n",
        "##**Quick notation note**\n",
        "\n",
        "In the past lecture, I defined the derivative with respect to $\\theta$ as:\n",
        "\n",
        "$\\frac{d}{d\\theta}$\n",
        "\n",
        "I would like to define it in a different way, using the gradient symbol:\n",
        "\n",
        "$\\nabla_\\theta$\n",
        "\n",
        "They both mean the same thing, but from now on we will be using the gradient symbol notation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##**Vectors and Matrices**\n",
        "\n",
        "If you go back to the first lecture, we defined our parameters in a vector notation as follows:\n",
        "\n",
        "\\begin{bmatrix} \\theta_0 \\\\\\ \\theta_1 \\\\\\ \\theta_2 \\end{bmatrix}\n",
        "\n",
        "This would apply to our first example where we had a dataset for houses and our dataset contained 2 parameters, so our parameter vector contains three variables. **Our parameter vector will always have n + 1 variables**, where n is the number of features in our dataset.\n",
        "\n",
        "If we would like to run the derivative of our cost function **J($\\theta$)** with respect to $\\theta$, it would look like this:\n",
        "\n",
        "###**$\\nabla_\\theta J(\\theta) \t\\to \\begin{bmatrix} \\frac{dJ}{d\\theta_0} \\\\\\ \\frac{dJ}{d\\theta_1} \\\\\\ \\frac{dJ}{d\\theta_2} \\end{bmatrix}$**\n",
        "\n",
        "In this case we have a 3x1 matrix, but this also applies to matrices. So if we have a 2x2 matrix **A** as follows:\n",
        "\n",
        "###**A = $\\begin{bmatrix}a_{11} & a_{12}\\\\\n",
        "a_{21} & a_{22}\n",
        "\\end{bmatrix}$**\n",
        "\n",
        "and let's say we have a function **F(A)** where if we take the gradient with respect to a, it would be:\n",
        "\n",
        "###**$\\nabla_A F(A) = \\begin{bmatrix} \\frac{df}{dA_{11}} &  \\frac{df}{dA_{12}} \\\\\\ \\frac{df}{dA_{21}} &  \\frac{df}{dA_{22}} \\end{bmatrix}$**\n",
        "---\n",
        "##**Trace of a Matrix**\n",
        "\n",
        "\n",
        "For every **NxN** matrix that we have, we can define a few more things with our matrix **A**:\n",
        "\n",
        "###tr(**A**) = Sum of diagonal entries\n",
        "\n",
        "this is called the **trace of A**, which is the sum of the diagonal entries. The Trace of A is equal to the trace of the transpose of A:\n",
        "\n",
        "###tr(**A**) = tr(**$A^T$**).\n",
        "\n",
        "And if we have a fixed Matrix **B** and a function:\n",
        "\n",
        "###F(**A**) = tr(**AB**)\n",
        "\n",
        "the derivative of our function is:\n",
        "\n",
        "###**$\\nabla_A F(A) = B^T$**\n",
        "\n",
        "There are many more trace operator derivations we can use for our matrix mathematics.\n",
        "\n",
        "##**Deriving the Normal Equation**\n",
        "\n",
        "Now if we take the derivative of our cost function with respect to $\\theta$ and set it equal to 0, we can solve the equation to find where $\\theta$ is the minimum. The minimum is where $\\frac{d}{d\\theta}$ is equal to 0.\n",
        "\n",
        "Our cost function is **Mean Squared Error**:\n",
        "\n",
        "###**J($\\theta$) = $\\frac{1}{2n}$$\\sum_{i=1}^n (h_\\theta(x_i) - y_i)^2$**\n",
        "\n",
        "Let's where we define a matrix **X** that conatins all the training examples in our dataset and we can stack them up as rows:\n",
        "\n",
        "###**X = $\\begin{bmatrix} (x^1)^T \\\\\\ (x^2)^T \\\\\\ (x^3)^T \\\\\\ ... \\\\\\ (x^n)^T \\end{bmatrix}$**\n",
        "\n",
        "To expand on this newly defined matrix **X**, we are taking our original mean squared error cost function and rewriting the $h_\\theta(x_i)$ portion as a vector. We are writing our vector to include all the training examples from 1 to n, where is the number of training examples. In order to stack it as a one column matrix, we take the transpose of every example. So:\n",
        "\n",
        "###**X = $\\begin{bmatrix} (x^1)^T \\\\\\ (x^2)^T \\\\\\ (x^3)^T \\\\\\ ... \\\\\\ (x^n)^T \\end{bmatrix}$** = $\\sum_{i=1}^n (x_i)$\n",
        "\n",
        "so if we were to multiply our X matrix by our parameter vector $\\theta$:\n",
        "\n",
        "###**X$\\theta$ = $\\begin{bmatrix} (x^1)^T \\\\\\ (x^2)^T \\\\\\ (x^3)^T \\\\\\ ... \\\\\\ (x^n)^T \\end{bmatrix}$ * $\\begin{bmatrix} \\theta_0 \\\\\\ \\theta_1 \\\\\\ \\theta_2 \\\\\\ ... \\\\\\ \\theta_n \\end{bmatrix}$**\n",
        "\n",
        "we would get:\n",
        "\n",
        "###**$\\begin{bmatrix} (x^1)^T \\theta \\\\\\ (x^2)^T \\theta \\\\\\ (x^3)^T \\theta \\\\\\ ... \\\\\\ (x^n)^T \\theta \\end{bmatrix}$**\n",
        "\n",
        "which is just the vector of all the predictions made by the algorithm:\n",
        "\n",
        "###**$\\begin{bmatrix} h_\\theta(x_1) \\\\\\ h_\\theta(x_2) \\\\\\ h_\\theta(x_3) \\\\\\ ... \\\\\\ h_\\theta(x_n) \\end{bmatrix}$**\n",
        "\n",
        "So now we have proven that:\n",
        "\n",
        "###**$\\sum_{i=1}^n h_\\theta(x_i)$** = **$\\begin{bmatrix} h_\\theta(x_1) \\\\\\ h_\\theta(x_2) \\\\\\ h_\\theta(x_3) \\\\\\ ... \\\\\\ h_\\theta(x_n) \\end{bmatrix}$**\n",
        "\n",
        "We can also define a vector **y** that takes all the labels from the training example and stacks them into a column vector as follows:\n",
        "\n",
        "###**y = $\\begin{bmatrix} y^1 \\\\\\ y^2 \\\\\\ y^3 \\\\\\ ... \\\\\\ y^n \\end{bmatrix}$**\n",
        "\n",
        "So now let us rewrite our cost function in matrix format using our previous definitions for X, $\\theta$, and y:\n",
        "\n",
        "###**J($\\theta$) = $\\frac{1}{2n}$$ (X\\theta - y)^2$**\n",
        "\n",
        "and there is a property of matrices where the **multiplication of a matrix and its transpose is equal to the sum of each element squared**. We can use this to further our equation to:\n",
        "\n",
        "###**J($\\theta$) = $\\frac{1}{2n}$$ (X\\theta - y)^T(X\\theta - y)$**\n",
        "\n",
        "Now if we take the gradient of this equation with respect to $\\theta$, we get:\n",
        "\n",
        "###**$\\nabla_\\theta J(\\theta) = \\frac{1}{n}$$ X^T(X\\theta - y)$**\n",
        "\n",
        "To find the minimum of our cost function we set it equal to 0 and solver for $\\theta$:\n",
        "\n",
        "###**$\\frac{1}{n}$$ X^T(X\\theta - y) = 0$**\n",
        "\n",
        "And our normal equation becomes:\n",
        "\n",
        "###**$\\theta = (X^TX)^{-1} X^Ty$**\n",
        "\n",
        "So with this equation, we can plug in the values and perform the matrix math to solve for our parameter matrix $\\theta$ without having to run gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "##**Recap**\n",
        "\n",
        "We talked earlier about how we can use gradient descent to find the ideal values for our parameters that will minimize the loss of our cost function for linear regression, but we can also use the normal equation:\n",
        "\n",
        "###**$\\theta = (X^TX)^{-1} X^Ty$**\n",
        "\n",
        "where $\\theta$ is the column vector containing all the parameters used in our regression equation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dnk7tv0V5SFT"
      }
    }
  ]
}