{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Recap\n",
        "\n",
        "In the last lesson, we talked about **Gradient Descent**, which is an algorithm we use to choose parameters that minimize loss from our cost (loss) function. This will help increase the accuracy of our predictions.\n",
        " We mainly went over the derivation of using gradient descent for **linear regression**.\n",
        "\n",
        " In this lesson, we will talk about how we can use the normal equation in linear regression to bypass gradient descent and perform much quicker optimizations\n",
        "\n",
        "\n",
        "---\n",
        "##**Bypassing Gradient Descent in Linear Regression**\n",
        "\n",
        "The information I cover in this lecture apply to **Linear Regression Only!** If you take a look at the past couple of lectures, we break down linear regression to derive our gradient descent algorithm that we use to optimize our parameters $\\theta_j$. We talked about how batch gradient descent can be very slow for linear regression if we have a very large dataset. There is actually a way we can bypass gradient descent in linear regression and optimize our parameters in just one step, eliminating the need for using an iterative algorithm like gradient descent. This is called the **Normal Equation**.\n",
        "\n",
        "Note again, the normal equation is used for linear regression and linear regression only.\n",
        "\n",
        "##**Quick notation note**\n",
        "\n",
        "In the past lecture, I defined the derivative with respect to $\\theta$ as:\n",
        "\n",
        "$\\frac{d}{d\\theta}$\n",
        "\n",
        "I would like to define it in a different way, using the gradient symbol:\n",
        "\n",
        "$\\nabla_\\theta$\n",
        "\n",
        "They both mean the same thing, but from now on we will be using the gradient symbol notation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##**The Normal Equation**\n",
        "\n",
        "If you go back to the first lecture, we defined our parameters in a vector notation as follows:\n",
        "\n",
        "\\begin{bmatrix} \\theta_0 \\\\\\ \\theta_1 \\\\\\ \\theta_2 \\end{bmatrix}\n",
        "\n",
        "This would apply to our first example where we had a dataset for houses and our dataset contained 2 parameters, so our parameter vector contains three variables. **Our parameter vector will always have n + 1 variables**, where n is the number of features in our dataset.\n",
        "\n",
        "If we would like to run the derivative of our cost function **J($\\theta$)** with respect to $\\theta$, it would look like this:\n",
        "\n",
        "###**$\\nabla_\\theta J(\\theta) \t\\to \\begin{bmatrix} \\frac{dJ}{d\\theta_0} \\\\\\ \\frac{dJ}{d\\theta_1} \\\\\\ \\frac{dJ}{d\\theta_2} \\end{bmatrix}$**\n",
        "\n",
        "This also applies to matrices. So if we have a **NxN** matrix, it would be the same idea.\n",
        "\n",
        "Now if we take the derivative of our cost function with respect to $\\theta$ and set it equal to 0, we can solve the equation to find where $\\theta$ is the minimum."
      ],
      "metadata": {
        "id": "dnk7tv0V5SFT"
      }
    }
  ]
}